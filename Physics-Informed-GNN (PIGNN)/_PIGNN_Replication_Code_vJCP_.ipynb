{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d568b132-e7ea-4a74-9c3d-8b1180d9c89a",
   "metadata": {},
   "source": [
    "# Physics-Informed Graph Neural Network (PIGNN)\n",
    "\n",
    "This version includes the following updates: \n",
    "\n",
    "- K-fold Cross validation\n",
    "- SHAP Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc6b39-35f1-4083-be7a-3d61be0c48a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18425957-5d3b-4e4a-a240-84d1b89d74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "import random\n",
    "import shap\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix, precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "set_seed(seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ea135-20d8-43b8-bfd5-6702e9225665",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Architecture definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc22de3-59e6-40b2-8b99-4491b1890660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGELayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implements a single GraphSAGE layer.\n",
    "    \n",
    "    - Applies message passing by aggregating neighbor embeddings.\n",
    "    - Uses different linear transformations for each edge type.\n",
    "    - Combines neighbor embeddings with self embeddings and applies a ReLU activation.\n",
    "    \n",
    "    Args:\n",
    "        in_dim (int): Input feature dimension.\n",
    "        out_dim (int): Output feature dimension.\n",
    "        edge_dim (int): Number of edge types.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, edge_dim: int): \n",
    "        super().__init__()        \n",
    "        self.lin_neighbors = nn.ModuleList([nn.Linear(in_dim, out_dim, bias=True) for _ in range(edge_dim)])\n",
    "        self.lin_self = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def message_passing(self, x: torch.Tensor, adj_tensor: torch.Tensor):\n",
    "    \n",
    "        \"\"\"\n",
    "        Performs message passing by aggregating neighbor embeddings using adjacency matrices.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix of shape (batch_size, num_nodes, feature_dim).\n",
    "            adj_tensor (torch.Tensor): Adjacency tensor with multiple edge types.\n",
    "        Returns:\n",
    "            torch.Tensor: Aggregated neighbor embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, num_nodes, _ = x.shape\n",
    "        aggregated_neigh_embeds = []\n",
    "    \n",
    "        for i in range(adj_tensor.shape[3]):  \n",
    "            adj_matrix = adj_tensor[:, :, :, i]  \n",
    "            neigh_embeds_i = torch.bmm(adj_matrix, x) \n",
    "            neigh_embeds_i = self.lin_neighbors[i](neigh_embeds_i)\n",
    "            aggregated_neigh_embeds.append(neigh_embeds_i)\n",
    "\n",
    "        neigh_embeds = sum(aggregated_neigh_embeds)  \n",
    "        return neigh_embeds\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_tensor: torch.Tensor):\n",
    "       \n",
    "        \"\"\"\n",
    "        Forward pass of the GraphSAGE layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix.\n",
    "            adj_tensor (torch.Tensor): Adjacency tensor.   \n",
    "        Returns:\n",
    "            torch.Tensor: Output node representations.\n",
    "        \"\"\"\n",
    "        \n",
    "        neigh_embeds = self.message_passing(x, adj_tensor)\n",
    "        x_self = self.lin_self(x)\n",
    "        out = neigh_embeds + x_self  \n",
    "        return self.act(out)  \n",
    "\n",
    "class GraphSAGEModel(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    GraphSAGE model with multiple layers for node representation learning.\n",
    "    \n",
    "    - Projects input features to a hidden space.\n",
    "    - Applies three GraphSAGE layers with ReLU activation and dropout.\n",
    "    - Outputs final node embeddings.\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Input feature dimension.\n",
    "        hidden_size (int): Hidden layer size.\n",
    "        out_features (int): Output feature dimension.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, hidden_size: int, out_features: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(in_features, hidden_size, bias=True)        \n",
    "        self.conv1 = GraphSAGELayer(in_dim=hidden_size, out_dim=hidden_size, edge_dim=16)\n",
    "        self.conv2 = GraphSAGELayer(in_dim=hidden_size, out_dim=hidden_size, edge_dim=16)\n",
    "        self.conv3 = GraphSAGELayer(in_dim=hidden_size, out_dim=hidden_size, edge_dim=16)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=dropout)        \n",
    "        self.lin_out = nn.Linear(hidden_size, hidden_size, bias=True)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_tensor: torch.Tensor):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass of the GraphSAGE model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features.\n",
    "            adj_tensor (torch.Tensor): Adjacency tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Node embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        x = self.conv1(x, adj_tensor)  \n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv2(x, adj_tensor) \n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin_out(x)  \n",
    "        return x \n",
    "\n",
    "class DNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Deep Neural Network (DNN) for path prediction.\n",
    "    \n",
    "    - Consists of five fully connected layers with ReLU activations.\n",
    "    - Outputs a probability score using a sigmoid activation.\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Input feature dimension.\n",
    "        hidden_size (int): Hidden layer size.\n",
    "        out_features (int): Output feature dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        \"\"\"\n",
    "        Forward pass of the DNN.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input features.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output probabilities.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))  \n",
    "        return x\n",
    "\n",
    "class GraphSAGEWithDNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Combines GraphSAGE and DNN for path prediction.\n",
    "    \n",
    "    - First extracts node embeddings using GraphSAGE.\n",
    "    - Then applies DNN to predict paths.\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Input feature dimension.\n",
    "        hidden_size (int): Hidden layer size.\n",
    "        out_features (int): Output feature dimension.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size, out_features, dropout=0):\n",
    "        super().__init__()\n",
    "        self.graphsage = GraphSAGEModel(in_features, hidden_size, hidden_size, dropout)\n",
    "        self.dnn = DNN(hidden_size, hidden_size, out_features)\n",
    "\n",
    "    def forward(self, x, adj_tensor):\n",
    "    \n",
    "        \"\"\"\n",
    "        Forward pass of the combined model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features.\n",
    "            adj_tensor (torch.Tensor): Adjacency tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted paths.\n",
    "        \"\"\"\n",
    "        \n",
    "        node_embeddings = self.graphsage(x, adj_tensor)\n",
    "        output = self.dnn(node_embeddings)\n",
    "        return output\n",
    "\n",
    "class AE(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Autoencoder (AE) model\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): Number of hidden features in the GraphSAGE output.\n",
    "        out_features (int): Number of features in the input X_matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, 128),                  # Added (_,128) layer\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)  \n",
    "        )\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, out_features)                  # Added (_,128) layer\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass for the autoencoder.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, num_nodes, hidden_size).  \n",
    "        Returns:\n",
    "            Tensor: Reconstructed output of shape (batch_size, num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "class GraphSAGEWithAE(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    GraphSAGE model combined with an Autoencoder (AE) for feature learning.\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Number of input node features.\n",
    "        hidden_size (int): Number of hidden units in GraphSAGE.\n",
    "        out_features (int): Output feature size (same as X_matrix.shape[2]).\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size, out_features, dropout=0):\n",
    "        super().__init__()\n",
    "        self.graphsage = GraphSAGEModel(in_features, hidden_size, hidden_size, dropout)\n",
    "        self.autoencoder = AE(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, x, adj_tensor):\n",
    "    \n",
    "        \"\"\"\n",
    "        Forward pass through GraphSAGE followed by the autoencoder.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input feature matrix of shape (batch_size, num_nodes, in_features).\n",
    "            adj_tensor (Tensor): Adjacency matrix of shape (batch_size, num_nodes, num_nodes).\n",
    "        Returns:\n",
    "            Tensor: Reconstructed feature matrix of shape (batch_size, num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        \n",
    "        node_embeddings = self.graphsage(x, adj_tensor)\n",
    "        reconstruction = self.autoencoder(node_embeddings)\n",
    "        return reconstruction\n",
    "\n",
    "class EncoderWithClassifier(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encoder model with a classifier for binary node classification.\n",
    "    \n",
    "    Args:\n",
    "        graphsage (nn.Module): Pretrained GraphSAGE model.\n",
    "        pretrained_encoder (nn.Module): Pretrained encoder (Autoencoder's encoder part).\n",
    "        latent_dim (int): Size of the latent representation.\n",
    "        freeze (bool): If True, freezes GraphSAGE and encoder layers during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, graphsage: nn.Module, pretrained_encoder: nn.Module, latent_dim: int, freeze: bool):\n",
    "        super().__init__()\n",
    "        self.graphsage = graphsage \n",
    "        self.encoder = pretrained_encoder\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.graphsage.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, latent_dim),      \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(latent_dim, 1),   \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, adj_tensor):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass through GraphSAGE, encoder, and classifier.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input feature matrix of shape (batch_size, num_nodes, in_features).\n",
    "            adj_tensor (Tensor): Adjacency matrix of shape (batch_size, num_nodes, num_nodes).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Classification probabilities of shape (batch_size, num_nodes).\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, num_nodes, _ = x.shape  \n",
    "        node_embeddings = self.graphsage(x, adj_tensor)\n",
    "        node_embeddings = node_embeddings.view(batch_size * num_nodes, node_embeddings.shape[2])\n",
    "        latent_repr = self.encoder(node_embeddings)\n",
    "        classification_output = self.classifier(latent_repr)\n",
    "        classification_output = classification_output.view(batch_size, num_nodes)\n",
    "        return classification_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7ac36-75c7-42e5-9ab2-12f035a9071e",
   "metadata": {},
   "source": [
    "## 2. Dataload, Loss, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c6f15-98d8-4427-a8b8-9a33d9bfbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Custom dataset loader for graph data.\n",
    "        - Loads adjacency tensor, node features (X_matrix), and target adjacency matrix (Y_matrix).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pt')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx])\n",
    "        return data[\"adj_tensor\"], data[\"X_matrix\"], data[\"Y_matrix\"]\n",
    "\n",
    "\"\"\"\n",
    "Initializes dataset and splits it into training and testing sets.\n",
    "Creates data loaders for efficient batch processing during training.\n",
    "\"\"\"\n",
    "\n",
    "data_dir = \"_data_\"\n",
    "dataset = GraphDataset(data_dir)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                              worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                              generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, \n",
    "                             worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                             generator=torch.Generator().manual_seed(42))  \n",
    "\n",
    "def degree_loss(B):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the degree loss to enforce a single path structure in the adjacency matrix B.\n",
    "    \n",
    "    Args:\n",
    "        B (torch.Tensor): Adjacency matrix of shape (B, N, N)\n",
    "    Returns:\n",
    "        torch.Tensor: Degree loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_nodes, _ = B.shape\n",
    "    deg_out = B.sum(dim=-1)  \n",
    "    deg_in = B.sum(dim=-2)  \n",
    "    active_nodes = (deg_in > 0) | (deg_out > 0)  \n",
    "    num_start_nodes = (deg_in == 0) & (deg_out > 0)  \n",
    "    P_start = (num_start_nodes.sum(dim=-1) - 1) ** 2  \n",
    "    num_end_nodes = (deg_out == 0) & (deg_in > 0) \n",
    "    P_end = (num_end_nodes.sum(dim=-1) - 1) ** 2  \n",
    "    incorrect_intermediate = ((deg_in != 1) | (deg_out != 1)) & active_nodes \n",
    "    P_intermediate = incorrect_intermediate.sum(dim=-1) - 2 \n",
    "    P_intermediate = torch.clamp(P_intermediate, min=0)      \n",
    "    L_deg = P_start + P_end + P_intermediate\n",
    "    \n",
    "    return L_deg.float().mean()\n",
    "\n",
    "def cycle_loss(B, K=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Penalizes cycles in the predicted adjacency matrix B by computing matrix powers up to K.\n",
    "\n",
    "    Args:\n",
    "        B (torch.Tensor): Adjacency matrix of shape (B, N, N).\n",
    "        K (int, optional): Maximum power to compute. Defaults to 10.\n",
    "    Returns:\n",
    "        torch.Tensor: Cycle loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, num_nodes, _ = B.shape\n",
    "    B = B / (B.sum(dim=-1, keepdim=True) + 1e-6)\n",
    "    cycle_penalty = torch.zeros(batch_size, device=B.device)\n",
    "    B_power = torch.eye(num_nodes, device=B.device).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "    for k in range(1, K + 1):  \n",
    "        B_power = torch.bmm(B_power, B)  \n",
    "        diag_sum = torch.diagonal(B_power, dim1=-2, dim2=-1).sum(dim=-1)  \n",
    "        cycle_penalty += diag_sum / k \n",
    "\n",
    "    return cycle_penalty.mean()\n",
    "\n",
    "def connectivity_loss(B):\n",
    "    \n",
    "    \"\"\" \n",
    "    Encourages the graph to contain a single connected path structure, \n",
    "    verified via Laplacian eigenvalues and connected component analysis.\n",
    "    \n",
    "    Args:\n",
    "        B (torch.Tensor): Adjacency matrix of shape (batch_size, N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Path structure loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, num_nodes, _ = B.shape\n",
    "    deg_out = B.sum(dim=-1)  \n",
    "    D = torch.diag_embed(deg_out)\n",
    "    L = D - B\n",
    "    eigvals = torch.linalg.eigvals(L).real  \n",
    "    zero_threshold = 1e-5  \n",
    "    num_components = (eigvals.abs() < zero_threshold).sum(dim=-1)\n",
    "    path_length = (B.sum(dim=(-1, -2)) / 2).long() + 1 \n",
    "    expected_components = num_nodes - path_length + 1\n",
    "    component_penalty = (num_components - expected_components) ** 2 / expected_components**2\n",
    "    max_pl = path_length.max().item()\n",
    "    max_pl = min(max_pl, num_nodes) \n",
    "    batch_path_eigvals = torch.zeros((batch_size, max_pl), device=B.device) \n",
    "\n",
    "    for idx, pl in enumerate(path_length):\n",
    "        pl_val = min(int(pl.item()), num_nodes)  \n",
    "\n",
    "        eigvals_path = 2 - 2 * torch.cos(torch.arange(pl_val, device=B.device) * torch.pi / pl_val)\n",
    "        if pl_val > max_pl:\n",
    "            print(f\"Warning: pl_val ({pl_val}) exceeds max_pl ({max_pl}), skipping assignment\")\n",
    "            continue  \n",
    "\n",
    "        batch_path_eigvals[idx, :pl_val] = eigvals_path\n",
    "\n",
    "    sorted_eigvals = torch.sort(eigvals, dim=-1).values[:, :max_pl] \n",
    "    batch_path_eigvals = batch_path_eigvals[:, :max_pl]\n",
    "    spectral_penalty = ((sorted_eigvals - batch_path_eigvals) ** 2).mean(dim=-1) \n",
    "    \n",
    "    return (component_penalty + spectral_penalty).mean()\n",
    "\n",
    "def masked_bce_loss(pred, target):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes a masked Binary Cross-Entropy (BCE) loss with class imbalance handling.\n",
    "    \n",
    "    This function applies BCE loss only to a subset of the target values:  \n",
    "    - All positive (1) values are included.  \n",
    "    - A small fraction of negative (0) values are randomly sampled to reduce class imbalance.  \n",
    "    - A positive weight is applied to further adjust for the imbalance.  \n",
    "    \n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted logits of shape (B, N, N).\n",
    "        target (torch.Tensor): Ground truth labels of shape (B, N, N).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean masked BCE loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    mask = ((target != 0) | (torch.rand_like(target) < 0.001)).float()\n",
    "    target_w = target.clone()\n",
    "    target_w[target == 1] = (1/2.6615810451242892e-03)\n",
    "    target_w[target == 0] = 0.00001\n",
    "    target_w = target_w.to(device)\n",
    "    loss = F.binary_cross_entropy(pred, target, reduction='none', weight=target_w*mask)\n",
    "    loss = loss * mask \n",
    "    return loss.sum() / mask.sum() \n",
    "    \n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the architecture using ROC-AUC Metric\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()  \n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():  \n",
    "        for adj_tensor, x_matrix, y_matrix in test_dataloader:\n",
    "            adj_tensor, x_matrix, y_matrix = adj_tensor.to(device), x_matrix.to(device), y_matrix.to(device)\n",
    "            output = model(x_matrix, adj_tensor)  \n",
    "            all_outputs.append(output.cpu().numpy().flatten())\n",
    "            all_targets.append(y_matrix.cpu().numpy().flatten())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs).astype(int)\n",
    "    all_targets = np.concatenate(all_targets).astype(int)\n",
    "    \n",
    "    predictions = (all_outputs > 0.5).astype(int)\n",
    "    auc_roc = roc_auc_score(all_targets, all_outputs)\n",
    "    return auc_roc\n",
    "\n",
    "def compute_roc_auc_with_threshold(model, dataloader, device):\n",
    "    \n",
    "    \"\"\" \n",
    "    Compute ROC-AUC values (FPR, TPR, AUC score) and find the best threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, Y_matrix in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            Y_matrix = Y_matrix.to(device)\n",
    "            outputs = model(X_matrix, adj_tensor)  \n",
    "            probabilities = torch.sigmoid(outputs)  \n",
    "            all_probs.append(probabilities.cpu().numpy().flatten())  \n",
    "            all_labels.append(Y_matrix.cpu().numpy().flatten())  \n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    J_scores = tpr - fpr\n",
    "    best_idx = J_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return fpr, tpr, roc_auc, best_threshold\n",
    "\n",
    "def compute_roc_auc_AE_with_threshold(model, dataloader, device, target): \n",
    "    \n",
    "    \"\"\" \n",
    "    Compute ROC-AUC values and find the best threshold for AE classifiers. \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, _ in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            labels = X_matrix[:, :, -target].float() \n",
    "            classification_output = model(X_matrix, adj_tensor)  \n",
    "            all_labels.extend(labels.cpu().numpy().flatten())  \n",
    "            all_probs.extend(classification_output.cpu().numpy().flatten())  \n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    J_scores = tpr - fpr\n",
    "    best_idx = J_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return fpr, tpr, roc_auc, best_threshold\n",
    "\n",
    "def compute_f1_score(model, dataloader, device, best_threshold):\n",
    "   \n",
    "    \"\"\" \n",
    "    Compute F1 score using the best threshold from ROC-AUC computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, Y_matrix in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            Y_matrix = Y_matrix.to(device)\n",
    "\n",
    "            outputs = model(X_matrix, adj_tensor)  \n",
    "            probabilities = torch.sigmoid(outputs)  \n",
    "\n",
    "            all_probs.append(probabilities.cpu().numpy().flatten())  \n",
    "            all_labels.append(Y_matrix.cpu().numpy().flatten())  \n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    preds = (all_probs >= best_threshold).astype(int)\n",
    "    f1 = f1_score(all_labels, preds, average=\"weighted\")\n",
    "    return f1\n",
    "\n",
    "def compute_f1_score_AE(model, dataloader, device, target, best_threshold):\n",
    "    \n",
    "    \"\"\" \n",
    "    Compute F1 score for Autoencoder classification models.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, _ in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            labels = X_matrix[:, :, -target].float()  \n",
    "            classification_output = model(X_matrix, adj_tensor)  \n",
    "            all_labels.extend(labels.cpu().numpy().flatten())  \n",
    "            all_probs.extend(classification_output.cpu().numpy().flatten())  \n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    preds = (all_probs >= best_threshold).astype(int)\n",
    "    f1 = f1_score(all_labels, preds, average=\"weighted\")\n",
    "    return f1\n",
    "\n",
    "def compute_confusion_matrix(model, dataloader, device, threshold, is_autoencoder=False, target=1):\n",
    "    \n",
    "    \"\"\" \n",
    "    Compute confusion matrix and F1 score for a given model. \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, Y_matrix in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "\n",
    "            if is_autoencoder:\n",
    "                labels = X_matrix[:, :, -target]\n",
    "                classification_output = model(X_matrix, adj_tensor)\n",
    "                preds = (classification_output > threshold).float()\n",
    "            else:\n",
    "                Y_matrix = Y_matrix.to(device)\n",
    "                labels = Y_matrix\n",
    "                classification_output = model(X_matrix, adj_tensor)\n",
    "                preds = (classification_output > threshold).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\") \n",
    "    return cm, f1\n",
    "\n",
    "def plot_confusion_matrix(cm, f1, title=\"Confusion Matrix\", filename=None):\n",
    "    \n",
    "    \"\"\" \n",
    "    Plot confusion matrix with F1 score in title. \n",
    "    \"\"\"\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.heatmap(cm_normalized, cbar=False, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "                xticklabels=[\"Negative\", \"Positive\"], \n",
    "                yticklabels=[\"Negative\", \"Positive\"], \n",
    "                vmin=0, vmax=1)  \n",
    "\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"{title}, F1={f1:.4f}\")  \n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight', transparent=True)  \n",
    "    plt.show()\n",
    "\n",
    "def pinn_loss(output, Y_matrix, alpha, beta, zeta):\n",
    "    \n",
    "    \"\"\" \n",
    "    Computes the total loss including masked-BCE loss and physics-inspired penalties. \n",
    "    \"\"\"\n",
    "\n",
    "    Y_matrix = Y_matrix.float()\n",
    "    data_loss = masked_bce_loss(output, Y_matrix)    \n",
    "    L_deg = degree_loss(output)\n",
    "    L_cyc = cycle_loss(output)  \n",
    "    L_con = connectivity_loss(output)      \n",
    "    pinn_loss = alpha * L_deg + zeta*L_cyc + beta*L_con\n",
    "\n",
    "    total_loss = data_loss + pinn_loss \n",
    "    return total_loss\n",
    "\n",
    "    \n",
    "def path_pred_k_fold_cross_validation(model_class, \n",
    "                            name, \n",
    "                            dataset, \n",
    "                            pinnloss, \n",
    "                            k=5, \n",
    "                            device='cuda', \n",
    "                            alpha=1, \n",
    "                            beta=0.0001,\n",
    "                            lr=0.001,\n",
    "                            zeta=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform K-Fold Cross Validation.\n",
    "    - dataset: The full dataset used for training.\n",
    "    - k: Number of folds (K=5).\n",
    "    - device: Device (CPU/GPU).\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    all_auc_scores = []\n",
    "    all_f1_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "  \n",
    "        print(f\"Fold {fold+1}/{k}\")\n",
    "\n",
    "        if pinnloss is True:\n",
    "            print('[i] Using PINN Loss...')\n",
    "        else:\n",
    "            print('[i] Using MWBCE Loss...')\n",
    "        \n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, \n",
    "                                  batch_size=64, \n",
    "                                  shuffle=True, \n",
    "                                  worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                                  generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        test_loader = DataLoader(test_subset, \n",
    "                                  batch_size=64, \n",
    "                                  shuffle=False, \n",
    "                                  worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                                  generator=torch.Generator().manual_seed(42))\n",
    "        \n",
    "        model = model_class(in_features, hidden_size, out_features, dropout)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr) # LR -> 0.0001 for no-pinn, 0.001 for pinn\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.97)\n",
    "\n",
    "        train_losses = []\n",
    "        log_data = []\n",
    "        \n",
    "        for epoch in range(20):  \n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, (adj_tensor, X_matrix, Y_matrix) in enumerate(train_loader):\n",
    "                adj_tensor = adj_tensor.to(device)\n",
    "                X_matrix = X_matrix.to(device)\n",
    "                Y_matrix = Y_matrix.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_matrix, adj_tensor)\n",
    "\n",
    "                if pinnloss is True:\n",
    "                    loss = pinn_loss(output, Y_matrix.float(), alpha, beta, zeta)\n",
    "                else:\n",
    "                    loss = masked_bce_loss(output, Y_matrix.float())  \n",
    "                \n",
    "                try:\n",
    "                    loss.backward()\n",
    "                except torch.linalg.LinAlgError as e:\n",
    "                    print(f\"LinAlgError: {e}\")\n",
    "\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            scheduler.step()\n",
    "            log_data.append([epoch + 1, batch_idx + 1, loss.item()])\n",
    "            \n",
    "            fpr, tpr, auc_roc, best_threshold = compute_roc_auc_with_threshold(model, test_loader, device)\n",
    "            f1 = compute_f1_score(model, test_loader, device, best_threshold)  \n",
    "            print(f\"[i] Epoch {epoch+1}, \\tTrain Loss: {avg_train_loss:.8f}, \\tAUC: {auc_roc:.4f}, \\tF1: {f1:.4f}\")\n",
    "\n",
    "        fpr, tpr, auc_roc, best_threshold = compute_roc_auc_with_threshold(model, test_loader, device)\n",
    "        f1 = compute_f1_score(model, test_loader, device, best_threshold)  \n",
    "        all_auc_scores.append(auc_roc)\n",
    "        all_f1_scores.append(f1)\n",
    "\n",
    "        print(f\"Fold {fold+1} - AUC: {auc_roc:.4f}, F1: {f1:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"Exports/5_{name}_fold_{fold+1}_weights.pth\")\n",
    "\n",
    "    mean_auc = np.mean(all_auc_scores)\n",
    "    std_auc = np.std(all_auc_scores)\n",
    "    mean_f1 = np.mean(all_f1_scores)\n",
    "    std_f1 = np.std(all_f1_scores)\n",
    "\n",
    "    print(\"Final K-Fold Results:\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "    return mean_auc, std_auc, mean_f1, std_f1\n",
    "\n",
    "def node_pred_k_fold_cross_validation(dataset, mode, k=5, device='cuda'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform K-Fold Cross Validation.\n",
    "        - dataset: The full dataset used for training.\n",
    "        - k: Number of folds (K=5).\n",
    "        - device: Device (CPU/GPU).\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    all_auc_scores = []\n",
    "    all_f1_scores = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "  \n",
    "        print(f\"Fold {fold+1}/{k}\")\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, \n",
    "                                  batch_size=64, \n",
    "                                  shuffle=True, \n",
    "                                  worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                                  generator=torch.Generator().manual_seed(42))\n",
    "        \n",
    "        test_loader = DataLoader(test_subset, \n",
    "                                 batch_size=64, \n",
    "                                 shuffle=False, \n",
    "                                 worker_init_fn=lambda worker_id: np.random.seed(42 + worker_id), \n",
    "                                 generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "        graphsage_model = GraphSAGEModel(in_features, hidden_size, hidden_size, dropout).to(device)\n",
    "        graphsage_model.load_state_dict(torch.load(\"Weights/graphsage_weights.pth\"))  # Pre-trained\n",
    "        pretrained_ae = AE(hidden_size, in_features).to(device)  \n",
    "        pretrained_ae.load_state_dict(torch.load(\"Weights/autoencoder_weights.pth\"))  # Pre-trained\n",
    "        pretrained_encoder = pretrained_ae.encoder \n",
    "        \n",
    "        latent_dim = 9           \n",
    "        freeze = False  \n",
    "        classifier_model = EncoderWithClassifier(graphsage_model, pretrained_encoder, latent_dim, freeze).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(classifier_model.parameters(), lr=0.0001)\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        \n",
    "        num_epochs = 10\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):  \n",
    "            classifier_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, (adj_tensor, X_matrix, _) in enumerate(train_loader):\n",
    "                \n",
    "                adj_tensor = adj_tensor.to(device)\n",
    "                labels = X_matrix[:, :, -int(mode)].float()  \n",
    "                labels = labels.to(device)     \n",
    "                \n",
    "                X_matrix_mask = X_matrix.clone()  \n",
    "                X_matrix_mask[:, :, -int(mode)] = 0 \n",
    "                X_matrix_mask = X_matrix_mask.to(device)  \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                classification_output = classifier_model(X_matrix_mask, adj_tensor)  \n",
    "                loss = criterion(classification_output, labels)  \n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}, Classifier Train Loss: {total_loss / len(train_loader):.8f}\")\n",
    "        \n",
    "\n",
    "        fpr, tpr, auc_roc, best_threshold = compute_roc_auc_AE_with_threshold(classifier_model, test_loader, device, target=int(mode))\n",
    "        f1 = compute_f1_score_AE(classifier_model, test_loader, device, target=1, best_threshold=best_threshold)\n",
    "        all_auc_scores.append(auc_roc)\n",
    "        all_f1_scores.append(f1)\n",
    "        print(f\"Fold {fold+1} - AUC: {auc_roc:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        print('[+] Saving end-to-end model weights...')\n",
    "        torch.save(classifier_model.graphsage.state_dict(), f\"Exports/fold_{fold+1}_sn_graphsage_ae_classifier_weights.pth\")\n",
    "        torch.save(classifier_model.encoder.state_dict(), f\"Exports/fold_{fold+1}_sn_encoder_ae_classifier_weights.pth\")\n",
    "        torch.save(classifier_model.classifier.state_dict(), f\"Exports/fold_{fold+1}_sn_classifier_ae_classifier_weights.pth\")\n",
    "\n",
    "    mean_auc = np.mean(all_auc_scores)\n",
    "    std_auc = np.std(all_auc_scores)\n",
    "    mean_f1 = np.mean(all_f1_scores)\n",
    "    std_f1 = np.std(all_f1_scores)\n",
    "\n",
    "    print(\"Final K-Fold Results:\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "    return mean_auc, std_auc, mean_f1, std_f1\n",
    "\n",
    "def plot_roc_auc_comparison(model1, \n",
    "                            model2, \n",
    "                            model3,\n",
    "                            threshold3,\n",
    "                            model4, \n",
    "                            threshold4,\n",
    "                            dataloader, \n",
    "                            device, \n",
    "                            label1, \n",
    "                            label2, \n",
    "                            label3,\n",
    "                            label4):\n",
    "\n",
    "    \n",
    "    \"\"\" \n",
    "    Plot ROC-AUC curves for models on the same graph. \n",
    "    \"\"\"\n",
    "    \n",
    "    fpr1, tpr1, auc1, _ = compute_roc_auc_with_threshold(model1, dataloader, device)\n",
    "    fpr2, tpr2, auc2, _ = compute_roc_auc_with_threshold(model2, dataloader, device)\n",
    "    fpr3, tpr3, auc3 = compute_roc_auc_AE(model3, dataloader, device, target=1, threshold=threshold3)\n",
    "    fpr4, tpr4, auc4 = compute_roc_auc_AE(model4, dataloader, device,  target=2, threshold=threshold4)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr3, tpr3, color='red', lw=1, label=f\"{label3} (AUC = {auc3:.4f})\")\n",
    "    plt.plot(fpr4, tpr4, color='red', lw=1, linestyle='--', label=f\"{label4} (AUC = {auc4:.4f})\")\n",
    "    plt.plot(fpr1, tpr1, color='blue', lw=1, label=f\"{label1} (AUC = {auc1:.4f})\")\n",
    "    plt.plot(fpr2, tpr2, color='blue', linestyle=\"--\", lw=1, label=f\"{label2} (AUC = {auc2:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle=\"dotted\", lw=1, label=\"Baseline\") \n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\",  fontsize=10)\n",
    "    plt.ylabel(\"True Positive Rate\",  fontsize=10)\n",
    "    plt.legend(loc='lower right', fontsize=10, frameon=True, framealpha=1)\n",
    "    plt.grid(color='black', linestyle='-', linewidth=.5, alpha=.3)\n",
    "    plt.draw()\n",
    "    plt.box(False)    \n",
    "    plt.savefig('Exports/all_models_roc_auc_compare.jpeg', dpi=400, bbox_inches='tight', transparent=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_roc_auc_with_threshold(model, dataloader, device):\n",
    "    \n",
    "    \"\"\" \n",
    "    Compute ROC-AUC values (FPR, TPR, AUC score) and find the best threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, Y_matrix in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            Y_matrix = Y_matrix.to(device)\n",
    "            outputs = model(X_matrix, adj_tensor)  \n",
    "            probabilities = torch.sigmoid(outputs)  \n",
    "            all_probs.append(probabilities.cpu().numpy().flatten())  \n",
    "            all_labels.append(Y_matrix.cpu().numpy().flatten())  \n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    J_scores = tpr - fpr\n",
    "    best_idx = J_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return fpr, tpr, roc_auc, best_threshold\n",
    "\n",
    "def compute_roc_auc_AE_with_threshold(model, dataloader, device, target): \n",
    "    \n",
    "    \"\"\" \n",
    "    Compute ROC-AUC values and find the best threshold for AE classifiers. \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, _ in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            labels = X_matrix[:, :, -target].float() \n",
    "            classification_output = model(X_matrix, adj_tensor)  \n",
    "            all_labels.extend(labels.cpu().numpy().flatten())  \n",
    "            all_probs.extend(classification_output.cpu().numpy().flatten())  \n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    J_scores = tpr - fpr\n",
    "    best_idx = J_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    return fpr, tpr, roc_auc, best_threshold\n",
    "\n",
    "def compute_roc_auc_AE(model, dataloader, device, target, threshold): \n",
    "    \n",
    "    \"\"\" \n",
    "    Compute ROC-AUC values (FPR, TPR, AUC score) for a given model. \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for adj_tensor, X_matrix, _ in dataloader:\n",
    "            adj_tensor = adj_tensor.to(device)\n",
    "            X_matrix = X_matrix.to(device)\n",
    "            labels = X_matrix[:, :, -target].float() \n",
    "            classification_output = model(X_matrix, adj_tensor)  \n",
    "            all_labels.extend(labels.cpu().numpy().flatten())  \n",
    "            all_probs.extend(classification_output.cpu().numpy().flatten())  \n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    preds = (all_probs > threshold).astype(float)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f39c11-a7db-4627-9c80-d2ff2ab0508e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PIGNN K-Fold Training & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95eec5-4b9e-41bf-b025-3b67e1788943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First we train the PIGNN (using L_pinn) then we train the regular GNN\n",
    "Only the learning rates defer, each is optimal for their architecture. \n",
    "Other HPs are similar.\n",
    "\"\"\"\n",
    "\n",
    "# PIGNN\n",
    "mean_auc1, std_auc1, mean_f1, std_f1 = path_pred_k_fold_cross_validation(GraphSAGEWithDNN,\n",
    "                                                                         'GraphSAGEWithDNNPinn',\n",
    "                                                                         dataset,\n",
    "                                                                         True,\n",
    "                                                                         k=5, \n",
    "                                                                         device=device, \n",
    "                                                                         alpha=1, \n",
    "                                                                         beta=0.0001,\n",
    "                                                                         lr=0.001,\n",
    "                                                                         zeta=1)\n",
    "\n",
    "# Regular GNN\n",
    "mean_auc2, std_auc2, mean_f2, std_f2 = path_pred_k_fold_cross_validation(GraphSAGEWithDNN, \n",
    "                                                                         'GraphSAGEWithDNNNoPinn',\n",
    "                                                                         dataset, \n",
    "                                                                         False,\n",
    "                                                                         k=5, \n",
    "                                                                         device=device, \n",
    "                                                                         alpha=1, \n",
    "                                                                         beta=0.0001,\n",
    "                                                                         lr=0.0001,\n",
    "                                                                         zeta=1)\n",
    "\n",
    "# Fine-tune Initial Access Classifier\n",
    "mean_auc3, std_auc3, mean_f3, std_f3 = k_fold_cross_validation(dataset,\n",
    "                                                             mode=1, # Start node\n",
    "                                                             k=5, \n",
    "                                                             device=device)\n",
    "\n",
    "# Fine-tune Impact Classifier\n",
    "mean_auc4, std_auc4, mean_f4, std_f4 = k_fold_cross_validation(dataset,\n",
    "                                                             mode=2, # Start node\n",
    "                                                             k=5, \n",
    "                                                             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a85499-0d61-4db3-ac88-ab982efbd88c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Find optimal thresholds using Youden's J index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8136149-c762-4771-8df5-ec6804ff78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start by loading the four models : Path prediction with and without PINN loss, then the 2 classification blocks\n",
    "Don't forget to load the architectures & data loaders first (Blocks 1,2,7, 13, 14a-b)\n",
    "\"\"\"\n",
    "\n",
    "in_features = dataset[0][1].shape[1]      \n",
    "out_features = dataset[0][2].shape[1]    \n",
    "hidden_size = 512\n",
    "dropout = 0.1\n",
    "\n",
    "model = GraphSAGEWithDNN(in_features, hidden_size, out_features, dropout)\n",
    "model.load_state_dict(torch.load(\"Exports/pinn_PIGNN.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_nopinn = GraphSAGEWithDNN(in_features, hidden_size, out_features, dropout)\n",
    "model_nopinn.load_state_dict(torch.load(\"Exports/nopinn_PIGNN.pth\"))\n",
    "model_nopinn.to(device)\n",
    "model_nopinn.eval()\n",
    "\n",
    "graphsage_model_1 = GraphSAGEModel(in_features, hidden_size, hidden_size, dropout)  \n",
    "pretrained_ae_1 = AE(hidden_size, in_features)\n",
    "pretrained_encoder_1 = pretrained_ae_1.encoder   \n",
    "ae_sn = EncoderWithClassifier(graphsage_model_1, pretrained_encoder_1, latent_dim=9, freeze = False)\n",
    "ae_sn.graphsage.load_state_dict(torch.load(\"Exports/fold_1_sn_graphsage_ae_classifier_weights.pth\"))\n",
    "ae_sn.encoder.load_state_dict(torch.load(\"Exports/fold_1_sn_encoder_ae_classifier_weights.pth\"))\n",
    "ae_sn.classifier.load_state_dict(torch.load(\"Exports/fold_1_sn_classifier_ae_classifier_weights.pth\"))\n",
    "ae_sn.to(device)\n",
    "ae_sn.eval()\n",
    "\n",
    "graphsage_model_2 = GraphSAGEModel(in_features, hidden_size, hidden_size, dropout) \n",
    "pretrained_ae_2 = AE(hidden_size, in_features)\n",
    "pretrained_encoder_2 = pretrained_ae_2.encoder   \n",
    "ae_en = EncoderWithClassifier(graphsage_model_2, pretrained_encoder_2, latent_dim=9, freeze = False)\n",
    "ae_en.graphsage.load_state_dict(torch.load(\"Exports/fold_1_en_graphsage_ae_classifier_weights.pth\"))  \n",
    "ae_en.encoder.load_state_dict(torch.load(\"Exports/fold_1_en_encoder_ae_classifier_weights.pth\"))\n",
    "ae_en.classifier.load_state_dict(torch.load(\"Exports/fold_1_en_classifier_ae_classifier_weights.pth\"))\n",
    "ae_en.to(device)\n",
    "ae_en.eval()\n",
    "\n",
    "\n",
    "fpr1, tpr1, auc1, best_threshold1 = compute_roc_auc_with_threshold(model, test_dataloader, device)\n",
    "fpr2, tpr2, auc2, best_threshold2 = compute_roc_auc_with_threshold(model_nopinn, test_dataloader, device)\n",
    "fpr3, tpr3, auc3, best_threshold3 = compute_roc_auc_AE_with_threshold(ae_sn, test_dataloader, device, target=1)\n",
    "fpr4, tpr4, auc4, best_threshold4 = compute_roc_auc_AE_with_threshold(ae_en, test_dataloader, device, target=2)\n",
    "\n",
    "print(f\"[+] Model 1 Best Threshold: {best_threshold1:.8f} -> {auc1}\")\n",
    "print(f\"[+] Model 2 Best Threshold: {best_threshold2:.8f} -> {auc2}\")\n",
    "print(f\"[+] Model 3 Best Threshold: {best_threshold3:.8f} -> {auc3}\")\n",
    "print(f\"[+] Model 4 Best Threshold: {best_threshold4:.8f} -> {auc4}\")\n",
    "\n",
    "plot_roc_auc_comparison(model, \n",
    "                        model_nopinn, \n",
    "                        ae_sn, \n",
    "                        best_treshold3,\n",
    "                        ae_en, \n",
    "                        best_treshold4,\n",
    "                        test_dataloader, \n",
    "                        device, \n",
    "                        label1=r'$\\mathcal{M}_1,\\quad\\Psi = 1$'+f\"\\t\", \n",
    "                        label2=r'$\\mathcal{M}_1,\\quad\\Psi = 0$'+f\"\\t\", \n",
    "                        label3=r'$\\mathcal{M}_2,\\quad\\Psi = 0$'+f\"\\t\", \n",
    "                        label4=r'$\\mathcal{M}_3,\\quad\\Psi = 0$'+f\"\\t\")\n",
    "\n",
    "# Compute F1 scores using these thresholds\n",
    "f1_1 = compute_f1_score(model, test_dataloader, device, best_threshold1)\n",
    "f1_2 = compute_f1_score(model_nopinn, test_dataloader, device, best_threshold2)\n",
    "f1_3 = compute_f1_score_AE(ae_sn, test_dataloader, device, target=1, best_threshold=best_threshold3)\n",
    "f1_4 = compute_f1_score_AE(ae_en, test_dataloader, device, target=2, best_threshold=best_threshold4)\n",
    "\n",
    "# Print F1 scores\n",
    "print(f\"Model 1 (Path Prediction, PINN) - F1 Score: {f1_1:.4f}\")\n",
    "print(f\"Model 2 (Path Prediction, No PINN) - F1 Score: {f1_2:.4f}\")\n",
    "print(f\"Model 3 (AE Classifier SN) - F1 Score: {f1_3:.4f}\")\n",
    "print(f\"Model 4 (AE Classifier EN) - F1 Score: {f1_4:.4f}\")\n",
    "\n",
    "# Compute confusion matrices & F1 scores\n",
    "cm1, f1_1 = compute_confusion_matrix(model, test_dataloader, device, best_threshold1)\n",
    "cm2, f1_2 = compute_confusion_matrix(model_nopinn, test_dataloader, device, best_threshold2)\n",
    "cm3, f1_3 = compute_confusion_matrix(ae_sn, test_dataloader, device, best_threshold3, is_autoencoder=True, target=1)\n",
    "cm4, f1_4 = compute_confusion_matrix(ae_en, test_dataloader, device, best_threshold4, is_autoencoder=True, target=2)\n",
    "\n",
    "# Plot confusion matrices with F1 scores\n",
    "plot_confusion_matrix(cm1, f1_1, title=r'$\\mathcal{M}_1,\\quad\\Psi=1$', filename='Exports/kf_cm_m1_pinn.png')\n",
    "plot_confusion_matrix(cm2, f1_2, title=r'$\\mathcal{M}_1,\\quad\\Psi=0$', filename='Exports/kf_cm_m1_nopinn.png')\n",
    "plot_confusion_matrix(cm3, f1_3, title=r'$\\mathcal{M}_2,\\quad\\Psi=0$', filename='Exports/kf_cm_m3.png')\n",
    "plot_confusion_matrix(cm4, f1_4, title=r'$\\mathcal{M}_3,\\quad\\Psi=0$', filename='Exports/kf_cm_m4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6fc4b-82b6-4620-b8eb-62ae2effb393",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sensitivity Analysis using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b6521-f693-4f02-a6f2-3e7d80cbbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGraphSHAPExplainer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Simplified SHAP explainer for GraphSAGE models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the explainer.\n",
    "        \n",
    "        Args:\n",
    "            model: Your GraphSAGEWithDNN model\n",
    "            device: Device to run on ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def explain_feature_importance(self, adj_tensor, X_matrix, background_samples=10):\n",
    "        \"\"\"\n",
    "        Explain feature importance.\n",
    "        \n",
    "        Args:\n",
    "            adj_tensor: Adjacency tensor for the graph\n",
    "            X_matrix: Node feature matrix\n",
    "            background_samples: Number of background samples to use\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Feature importance scores\n",
    "        \"\"\"\n",
    "\n",
    "        adj_tensor = adj_tensor.to(self.device)\n",
    "        X_matrix = X_matrix.to(self.device)\n",
    "        batch_size, num_nodes, feature_dim = X_matrix.shape\n",
    "        \n",
    "        def model_wrapper(features):\n",
    "            \"\"\"\n",
    "            Wrapper function to handle feature perturbations\n",
    "            \"\"\"\n",
    "            \n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32).to(self.device)\n",
    "            batch_results = []\n",
    "            batch_size_shap = min(8, features_tensor.shape[0])\n",
    "            \n",
    "            for i in range(0, features_tensor.shape[0], batch_size_shap):\n",
    "                batch_features = features_tensor[i:i+batch_size_shap]\n",
    "                batch_features = batch_features.reshape(-1, num_nodes, feature_dim)                \n",
    "                batch_adj = adj_tensor[0:1].repeat(batch_features.shape[0], 1, 1, 1)                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_features, batch_adj)\n",
    "                if len(outputs.shape) > 2:\n",
    "                    outputs = outputs.mean(dim=(1, 2))\n",
    "                batch_results.append(outputs.cpu().numpy())\n",
    "            \n",
    "            return np.concatenate(batch_results, axis=0)\n",
    "        \n",
    "        background = []\n",
    "        base_features = X_matrix[0].cpu().numpy()        \n",
    "        for _ in range(background_samples):\n",
    "            noise = np.random.normal(0, 0.1, size=base_features.shape)\n",
    "            perturbed = base_features + noise\n",
    "            background.append(perturbed.flatten())\n",
    "        \n",
    "        background = np.array(background)\n",
    "        \n",
    "        print(\"Initializing SHAP Kernel explainer...\")\n",
    "        explainer = shap.KernelExplainer(\n",
    "            lambda x: model_wrapper(x.reshape(-1, num_nodes * feature_dim)), \n",
    "            background\n",
    "        )\n",
    "        print(\"Calculating SHAP values...\")\n",
    "        shap_values = explainer.shap_values(base_features.flatten().reshape(1, -1))\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0].reshape(num_nodes, feature_dim)\n",
    "        else:\n",
    "            shap_values = shap_values.reshape(num_nodes, feature_dim)\n",
    "        \n",
    "        return shap_values\n",
    "    \n",
    "    def explain_edge_importance(self, adj_tensor, X_matrix, background_samples=10):\n",
    "        \n",
    "        \"\"\"\n",
    "        Explain edge type importance.\n",
    "        \n",
    "        Args:\n",
    "            adj_tensor: Adjacency tensor for the graph\n",
    "            X_matrix: Node feature matrix\n",
    "            background_samples: Number of background samples to use\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Edge type importance scores\n",
    "        \"\"\"\n",
    "        \n",
    "        adj_tensor = adj_tensor.to(self.device)\n",
    "        X_matrix = X_matrix.to(self.device)\n",
    "        num_edge_types = adj_tensor.shape[3]\n",
    "        \n",
    "        def model_wrapper(edge_masks):\n",
    "            \"\"\"\n",
    "            Wrapper function to handle edge type masking\n",
    "            \"\"\"\n",
    "        \n",
    "            edge_masks = torch.tensor(edge_masks, dtype=torch.float32).to(self.device)\n",
    "            batch_results = []\n",
    "            batch_size_shap = min(8, edge_masks.shape[0])\n",
    "            \n",
    "            for i in range(0, edge_masks.shape[0], batch_size_shap):\n",
    "                batch_masks = edge_masks[i:i+batch_size_shap]                \n",
    "                batch_adjs = []\n",
    "                for j in range(len(batch_masks)):\n",
    "                    mask = batch_masks[j].reshape(1, 1, 1, -1)\n",
    "                    masked_adj = adj_tensor[0:1] * mask\n",
    "                    batch_adjs.append(masked_adj)\n",
    "                    \n",
    "                stacked_adj = torch.cat(batch_adjs, dim=0)\n",
    "                batch_X = X_matrix[0:1].repeat(len(batch_masks), 1, 1)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_X, stacked_adj)    \n",
    "                if len(outputs.shape) > 2:\n",
    "                    outputs = outputs.mean(dim=(1, 2))\n",
    "                \n",
    "                batch_results.append(outputs.cpu().numpy())\n",
    "            \n",
    "            return np.concatenate(batch_results, axis=0)\n",
    "        \n",
    "        background = np.random.randint(0, 2, size=(background_samples, num_edge_types)).astype(float)\n",
    "        background[0] = np.ones(num_edge_types)        \n",
    "        print(\"Initializing SHAP Kernel explainer for edge types...\")\n",
    "        explainer = shap.KernelExplainer(model_wrapper, background)        \n",
    "        print(\"Calculating SHAP values for edge types...\")\n",
    "        edge_type_masks = np.eye(num_edge_types)\n",
    "        shap_values = explainer.shap_values(edge_type_masks)        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        return shap_values\n",
    "    \n",
    "    def visualize_results(self, feature_shap, edge_shap):\n",
    "        \n",
    "        \"\"\"\n",
    "        Visualize the SHAP results.\n",
    "        \n",
    "        Args:\n",
    "            feature_shap: Feature importance scores\n",
    "            edge_shap: Edge type importance scores\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))        \n",
    "        feature_importance = np.abs(feature_shap).mean(axis=0)\n",
    "        feature_names = [f\"Feature {i}\" for i in range(len(feature_importance))]\n",
    "        idx = np.argsort(-feature_importance)\n",
    "        feature_importance = feature_importance[idx]\n",
    "        feature_names = [feature_names[i] for i in idx]        \n",
    "        plt.barh(feature_names[:10], feature_importance[:10])\n",
    "        plt.xlabel('Average |SHAP Value|')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Top 10 Node Features by Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('node_feature_importance.png')\n",
    "        plt.close()        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if len(edge_shap.shape) > 1:\n",
    "            edge_importance = np.abs(edge_shap).mean(axis=0)\n",
    "        else:\n",
    "            edge_importance = np.abs(edge_shap)\n",
    "        \n",
    "        edge_names = [f\"Edge Type {i}\" for i in range(len(edge_importance))]        \n",
    "        idx = np.argsort(-edge_importance)\n",
    "        edge_importance = edge_importance[idx]\n",
    "        edge_names = [edge_names[i] for i in idx]\n",
    "        plt.barh(edge_names, edge_importance)\n",
    "        plt.xlabel('Average |SHAP Value|')\n",
    "        plt.ylabel('Edge Type')\n",
    "        plt.title('Edge Types by Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('edge_type_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Visualizations saved to 'node_feature_importance.png' and 'edge_type_importance.png'\")\n",
    "\n",
    "def explain_graph_model_simple(model, dataloader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the simplified SHAP explainer.\n",
    "    Args:\n",
    "        model: Your GraphSAGEWithDNN model\n",
    "        dataloader: DataLoader with your graph data\n",
    "    \"\"\"\n",
    "    explainer = SimpleGraphSHAPExplainer(model)\n",
    "    batch_data = next(iter(dataloader))\n",
    "    adj_tensor, X_matrix, _ = batch_data\n",
    "    adj_tensor = adj_tensor[:1]  \n",
    "    X_matrix = X_matrix[:1]\n",
    "    print(f\"adj_tensor shape: {adj_tensor.shape}\")\n",
    "    print(f\"X_matrix shape: {X_matrix.shape}\")\n",
    "    print(\"Explaining feature importance...\")\n",
    "    feature_shap = explainer.explain_feature_importance(adj_tensor, X_matrix)\n",
    "    print(\"Explaining edge type importance...\")\n",
    "    edge_shap = explainer.explain_edge_importance(adj_tensor, X_matrix)\n",
    "    print(\"Visualizing results...\")\n",
    "    explainer.visualize_results(feature_shap, edge_shap)\n",
    "    \n",
    "    return {\n",
    "        'feature_shap': feature_shap,\n",
    "        'edge_shap': edge_shap\n",
    "    }\n",
    "\n",
    "class ClassifierGraphSHAPExplainer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Optimized SHAP explainer for EncoderWithClassifier models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device=None):\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def explain_feature_importance(self, adj_tensor, X_matrix, target_nodes=None, background_samples=5, n_samples=50):\n",
    "        adj_tensor, X_matrix = adj_tensor.to(self.device), X_matrix.to(self.device)\n",
    "        batch_size, num_nodes, feature_dim = X_matrix.shape\n",
    "\n",
    "        if target_nodes is None:\n",
    "            selected_node_indices = np.linspace(0, num_nodes-1, min(num_nodes, 50), dtype=int) # 5->50\n",
    "        else:\n",
    "            selected_node_indices = np.array(target_nodes)\n",
    "\n",
    "        def model_wrapper(features):\n",
    "            features_tensor = torch.tensor(features, dtype=torch.float32).to(self.device)\n",
    "            batch_results = []\n",
    "            \n",
    "            for i in range(0, features_tensor.shape[0], 4): # 4-> 49\n",
    "                batch_features = features_tensor[i:i+4] # 4 -> 49\n",
    "                batch_X = X_matrix[0:1].repeat(batch_features.shape[0], 1, 1)\n",
    "                for j, node_idx in enumerate(selected_node_indices):\n",
    "                    batch_X[:, node_idx, :] = batch_features[:, j*feature_dim:(j+1)*feature_dim]\n",
    "                batch_adj = adj_tensor[0:1].repeat(batch_features.shape[0], 1, 1, 1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_X, batch_adj)\n",
    "                batch_results.append(outputs[:, selected_node_indices].cpu().numpy())\n",
    "            \n",
    "            return np.concatenate(batch_results, axis=0)\n",
    "        \n",
    "        base_features = X_matrix[0, selected_node_indices].cpu().numpy().reshape(-1)\n",
    "        background = [base_features + np.random.normal(0, 0.05, base_features.shape) for _ in range(background_samples)]\n",
    "        \n",
    "        explainer = shap.KernelExplainer(model_wrapper, np.array(background), link=\"identity\")\n",
    "        shap_values = explainer.shap_values(base_features.reshape(1, -1), nsamples=n_samples, l1_reg=\"num_features(10)\")\n",
    "        \n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[0]\n",
    "        \n",
    "        full_shap_values = np.zeros((num_nodes, feature_dim))\n",
    "        for i, node_idx in enumerate(selected_node_indices):\n",
    "            node_shap_values = shap_values[0, i*feature_dim:(i+1)*feature_dim]\n",
    "            \n",
    "            if len(node_shap_values.shape) > 1:\n",
    "                node_shap_values = node_shap_values.mean(axis=-1)\n",
    "                \n",
    "            full_shap_values[node_idx, :] = node_shap_values\n",
    "        \n",
    "        return full_shap_values\n",
    "    \n",
    "    def explain_edge_importance(self, adj_tensor, X_matrix, target_nodes=None, background_samples=5, n_samples=500): # samples 50->500\n",
    "        adj_tensor, X_matrix = adj_tensor.to(self.device), X_matrix.to(self.device)\n",
    "        _, num_nodes, _, num_edge_types = adj_tensor.shape\n",
    "\n",
    "        if target_nodes is None:\n",
    "            selected_node_indices = np.linspace(0, num_nodes-1, min(num_nodes, 50), dtype=int) # 5 -> 50\n",
    "        else:\n",
    "            selected_node_indices = np.array(target_nodes)\n",
    "\n",
    "        def model_wrapper(edge_masks):\n",
    "            edge_masks = torch.tensor(edge_masks, dtype=torch.float32).to(self.device)\n",
    "            batch_results = []\n",
    "            \n",
    "            for i in range(0, edge_masks.shape[0], 4): # 4 -> 49\n",
    "                batch_masks = edge_masks[i:i+4]\n",
    "                batch_adjs = [adj_tensor[0:1] * mask.reshape(1, 1, 1, -1) for mask in batch_masks]\n",
    "                stacked_adj = torch.cat(batch_adjs, dim=0)\n",
    "                batch_X = X_matrix[0:1].repeat(len(batch_masks), 1, 1)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_X, stacked_adj)\n",
    "                batch_results.append(outputs[:, selected_node_indices].cpu().numpy())\n",
    "            \n",
    "            return np.concatenate(batch_results, axis=0)\n",
    "        \n",
    "        background = np.zeros((background_samples, num_edge_types))\n",
    "        background[0] = np.ones(num_edge_types)\n",
    "        for i in range(1, background_samples):\n",
    "            background[i] = np.random.choice([0, 1], size=num_edge_types, p=[0.3, 0.7])\n",
    "            if np.sum(background[i]) == 0:\n",
    "                background[i][np.random.randint(0, num_edge_types)] = 1\n",
    "        \n",
    "        explainer = shap.KernelExplainer(model_wrapper, background, link=\"identity\")\n",
    "        shap_values = explainer.shap_values(np.eye(num_edge_types), nsamples=n_samples, l1_reg=\"num_features(2)\")\n",
    "        \n",
    "        return shap_values[0] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "feature_names = ['Computer', 'OU', 'User', 'Group', 'GPO', 'Domain', \"Windows Server 2003\", \"Windows Server 2008\", \n",
    "                \"Windows 7\", \"Windows 10\", \"Windows XP\", \"Windows Server 2012\", \"Windows Server 2008\", \n",
    "                \"enabled\", \"hasspn\", \"highvalue\", \"is_vulnerable\", \"End Node\", \"Start Node\"]\n",
    "\n",
    "edge_types = [\"AdminTo\", \"AllowedToDelegate\", \"CanRDP\", \"Contains\", \"DCSync\", \"ExecuteDCOM\", \n",
    "              \"GenericAll\", \"GetChanges\", \"GetChangesAll\", \"GpLink\", \"HasSession\", \"MemberOf\", \n",
    "              \"Open\", \"Owns\", \"WriteDacl\", \"WriteOwner\"]\n",
    "\n",
    "def plot_importance_values(importance_matrix, \n",
    "                           names, \n",
    "                           title=\"Importance Values\", \n",
    "                           figsize=(12, 10), \n",
    "                           scale_method=\"normalize\", \n",
    "                           max_features=20):\n",
    "    \"\"\"\n",
    "    Plot feature or edge importance values.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(importance_matrix.shape) == 1:\n",
    "        importance_matrix = importance_matrix.reshape(1, -1)\n",
    "        n_items = importance_matrix.shape[1]\n",
    "    if len(names) < n_items:\n",
    "        names = list(names) + [f\"Unknown {i}\" for i in range(len(names), n_items)]\n",
    "    \n",
    "    mean_importance = importance_matrix.mean(axis=0)\n",
    "    sorted_indices = np.argsort(-mean_importance)  # High values at top    \n",
    "    top_n = min(max_features, n_items)\n",
    "    sorted_indices = sorted_indices[:top_n]    \n",
    "    plot_data = []\n",
    "    item_names = []\n",
    "    \n",
    "    for idx in sorted_indices:\n",
    "        values = importance_matrix[:, idx]\n",
    "        plot_data.append(values)\n",
    "        item_names.append(names[idx] if idx < len(names) else f\"Unknown {idx}\")\n",
    "    \n",
    "    all_values = np.concatenate([values for values in plot_data])\n",
    "    \n",
    "    if scale_method == \"normalize\":\n",
    "        max_abs = np.max(np.abs(all_values))\n",
    "        if max_abs > 0:\n",
    "            scaled_plot_data = [values / max_abs for values in plot_data]\n",
    "        else:\n",
    "            scaled_plot_data = plot_data\n",
    "        x_label = \"Importance\"\n",
    "\n",
    "    elif scale_method == \"percentile\":\n",
    "        pos_values = all_values[all_values > 0]\n",
    "        neg_values = all_values[all_values < 0]\n",
    "        \n",
    "        def scale_by_percentile(x, values):\n",
    "            if len(values) == 0:\n",
    "                return x\n",
    "            percentiles = np.percentile(values, [25, 50, 75, 95])\n",
    "            if x <= percentiles[0]:\n",
    "                return x / percentiles[0] * 0.25\n",
    "            elif x <= percentiles[1]:\n",
    "                return 0.25 + (x - percentiles[0]) / (percentiles[1] - percentiles[0]) * 0.25\n",
    "            elif x <= percentiles[2]:\n",
    "                return 0.5 + (x - percentiles[1]) / (percentiles[2] - percentiles[1]) * 0.25\n",
    "            elif x <= percentiles[3]:\n",
    "                return 0.75 + (x - percentiles[2]) / (percentiles[3] - percentiles[2]) * 0.2\n",
    "            else:\n",
    "                return 0.95 + (x - percentiles[3]) / (np.max(values) - percentiles[3]) * 0.05\n",
    "        \n",
    "        scaled_plot_data = []\n",
    "        for values in plot_data:\n",
    "            scaled_values = np.zeros_like(values, dtype=float)\n",
    "            for i, v in enumerate(values):\n",
    "                if v > 0:\n",
    "                    scaled_values[i] = scale_by_percentile(v, pos_values)\n",
    "                elif v < 0:\n",
    "                    scaled_values[i] = -scale_by_percentile(abs(v), np.abs(neg_values))\n",
    "            scaled_plot_data.append(scaled_values)\n",
    "        \n",
    "        x_label = \"Percentile-based Importance\"\n",
    "        \n",
    "    elif scale_method == \"robust\":\n",
    "        q1, q3 = np.percentile(np.abs(all_values), [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:  \n",
    "            iqr = 1.0\n",
    "        \n",
    "        scaled_plot_data = []\n",
    "        for values in plot_data:\n",
    "            scaled_values = np.zeros_like(values, dtype=float)\n",
    "            for i, v in enumerate(values):\n",
    "                if v != 0:\n",
    "                    scaled_values[i] = v / iqr * np.sign(v)\n",
    "            scaled_plot_data.append(scaled_values)\n",
    "        \n",
    "        x_label = \"Robust Scaled Importance (IQR normalized)\"\n",
    "    \n",
    "    else:\n",
    "        scaled_plot_data = plot_data\n",
    "        x_label = \"Raw Importance Value\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)    \n",
    "    positions = np.arange(len(item_names))    \n",
    "    for i, values in enumerate(scaled_plot_data):\n",
    "        y_pos = np.ones_like(values) * positions[i]        \n",
    "        pos_mask = values >= 0\n",
    "        neg_mask = values < 0        \n",
    "        if np.any(pos_mask):\n",
    "            ax.scatter(values[pos_mask], y_pos[pos_mask], color='red', alpha=0.25, s=100)\n",
    "        if np.any(neg_mask):\n",
    "            ax.scatter(values[neg_mask], y_pos[neg_mask], color='blue', alpha=0.25, s=100)\n",
    "    \n",
    "    for i, values in enumerate(scaled_plot_data):\n",
    "        mean_val = np.mean(values)\n",
    "        ax.plot([mean_val, mean_val], [positions[i]-0.4, positions[i]+0.4], color='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_yticks(positions)\n",
    "    ax.set_yticklabels(item_names, fontsize=20)\n",
    "    ax.set_xlabel(x_label, fontsize=20)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    \n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.3)    \n",
    "    if scale_method == \"normalize\":\n",
    "        ax.set_xlim(-1.1, 1.1)\n",
    "        ax.set_xticks([-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1])\n",
    "        ax.tick_params(axis='x', labelsize=20) \n",
    "        \n",
    "    elif scale_method == \"percentile\":\n",
    "        ax.set_xlim(-1.1, 1.1)\n",
    "        ax.set_xticks([-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', label='Positive Target', markersize=12),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', label='Negative Target', markersize=12),\n",
    "        Line2D([0], [0], color='black', label='Mean')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801607f4-c095-49d2-b6dd-dfc323c717d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the explainer for PIGNN - model must be declared first\n",
    "Then we plot the SHAP values for features and edges\n",
    "\"\"\"\n",
    "\n",
    "results = explain_graph_model_simple(model, test_dataloader)\n",
    "feature_importance = results['feature_shap']\n",
    "edge_importance = results['edge_shap']\n",
    "\n",
    "feature_avg = np.abs(feature_importance).mean(axis=0)\n",
    "top_features = np.argsort(-feature_avg)[:5]\n",
    "print(\"Top 5 features:\", top_features)\n",
    "\n",
    "edge_avg = np.abs(edge_importance).mean(axis=0) if len(edge_importance.shape) > 1 else np.abs(edge_importance)\n",
    "top_edges = np.argsort(-edge_avg)[:5]\n",
    "print(\"Top 5 edge types:\", top_edges)\n",
    "\n",
    "#Eventually save output\n",
    "np.save('Exports/shap_nodefeature.npy', feature_importance)\n",
    "np.save('Exports/shap_edgetype.npy', edge_importance)\n",
    "\n",
    "fig1, ax1 = plot_importance_values(\n",
    "    feature_importance, \n",
    "    names=feature_names,\n",
    "    title=r'$\\mathcal{M}_1\\,(\\Psi=1)$'+\"  Feature SHAP Values\",\n",
    "    scale_method=\"normalize\"\n",
    ")\n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Exports/pinn_feature_shap.png\", dpi=400, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "if len(edge_importance.shape) > 1:\n",
    "    fig2, ax2 = plot_importance_values(\n",
    "        edge_importance, \n",
    "        names=edge_types,\n",
    "        title=r'$\\mathcal{M}_1\\,(\\Psi=1)$'+\"  Edge Type SHAP Values\",\n",
    "        scale_method=\"normalize\")\n",
    "else:\n",
    "    edge_imp_reshaped = edge_importance.reshape(1, -1)\n",
    "    fig2, ax2 = plot_importance_values(\n",
    "        edge_imp_reshaped,\n",
    "        names=edge_types,\n",
    "        title=r'$\\mathcal{M}_1\\,(\\Psi=1)$'+\"  Edge Type Importance Distribution\",\n",
    "        scale_method=\"normalize\")\n",
    "    \n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Exports/pinn_edge_shap.png\", dpi=400, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Now we compute and plot SHAP values for M2 and M3, features and edges\n",
    "Start with Impact Node (EN)\n",
    "\"\"\"\n",
    "\n",
    "explainer_en = ClassifierGraphSHAPExplainer(ae_en, device)\n",
    "adj_tensor, X_matrix, _ = next(iter(test_dataloader))\n",
    "\n",
    "feature_importance_en = explainer_en.explain_feature_importance(\n",
    "    adj_tensor, \n",
    "    X_matrix)\n",
    "\n",
    "edge_importance_en = explainer_en.explain_edge_importance(\n",
    "    adj_tensor, \n",
    "    X_matrix)\n",
    "\n",
    "# Eventually save output\n",
    "np.save('feature_importance_en.npy', feature_importance_en)\n",
    "np.save('edge_importance_en.npy', edge_importance_en)\n",
    "\n",
    "fig, ax = plot_importance_values(feature_importance_en, names=feature_names, title=r'$\\mathcal{M}_3$'+\"  Feature SHAP Values\")\n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.savefig('en_feature_shap.jpeg', dpi=400, bbox_inches='tight', transparent=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "edge_importance_en_avg = edge_importance_en.mean(axis=2)  \n",
    "fig, ax = plot_importance_values(edge_importance_en_avg, names=edge_types, title=r'$\\mathcal{M}_3$'+\"  Edge Type SHAP Values\")\n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.savefig('en_edge_shap.jpeg', dpi=400, bbox_inches='tight', transparent=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now for Initial Access Model (SN)\n",
    "\n",
    "explainer_sn = ClassifierGraphSHAPExplainer(ae_sn, device)\n",
    "\n",
    "feature_importance_sn = explainer_sn.explain_feature_importance(\n",
    "    adj_tensor, \n",
    "    X_matrix)\n",
    "\n",
    "edge_importance_sn = explainer_sn.explain_edge_importance(\n",
    "    adj_tensor, \n",
    "    X_matrix)\n",
    "\n",
    "fig, ax = plot_importance_values(feature_importance_sn, names=feature_names, title=r'$\\mathcal{M}_2$'+\"  Feature SHAP Values\")\n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.savefig('sn_feature_shap.jpeg', dpi=400, bbox_inches='tight', transparent=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "edge_importance_sn_avg = edge_importance_sn.mean(axis=2)  # Shape becomes (16, 16)\n",
    "fig, ax = plot_importance_values(edge_importance_sn_avg, names=edge_types, title=r'$\\mathcal{M}_2$'+\"  Edge Type SHAP Values\")\n",
    "plt.draw()\n",
    "plt.box(False)    \n",
    "plt.savefig('sn_edge_shap.jpeg', dpi=400, bbox_inches='tight', transparent=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
